{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some startup code\n",
    "\n",
    "import packages needed & load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2825: DtypeWarning: Columns (0,1,2,3,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data  : 49999 rows\n",
      "evaluation data: 49999 rows\n",
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets, preprocessing\n",
    "import numpy as np\n",
    "from loaddata import get_instances_from_csv\n",
    "from startEvaluation import evaluation\n",
    "\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "DATAPATH = \"../sub_datasets/subset_0.csv\"\n",
    "DATAPATH1 = \"../sub_datasets/subset_1.csv\"\n",
    "RANDOM_SEED=42\n",
    "TRAINROWS = 50000\n",
    "EVALROWS = 50000\n",
    "\n",
    "training_data = get_instances_from_csv(DATAPATH, \"train\", numrows=TRAINROWS)\n",
    "eval_data = get_instances_from_csv(DATAPATH1,\"train\", numrows=EVALROWS)\n",
    "print(\"training data  : %s rows\"%training_data[0].shape[0])\n",
    "print(\"evaluation data: %s rows\"%eval_data[0].shape[0])\n",
    "\n",
    "kernels = ['rbf', 'linear', 'sigmoid', 'poly']\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(training_data[0])\n",
    "print(scaler)\n",
    "\n",
    "def print_prediction(training_data, eval_data, *args, **kwargs):\n",
    "\n",
    "    print(\"start training\")\n",
    "    svc = svm.SVC(random_state=RANDOM_SEED, cache_size=2000, max_iter=2000, *args, **kwargs).fit(training_data[0], training_data[1])\n",
    "    print(\"finished training svm %s\"%svc)\n",
    "    evt = evaluation(training_data[1], svc.predict(training_data[0]))\n",
    "    print(\"accuracy on traning data:\")\n",
    "    evt.print_only_accuracy()\n",
    "    eve = evaluation(eval_data[1], svc.predict(eval_data[0]))\n",
    "    print(\"accuracy on evaluation data:\")\n",
    "    eve.print_only_accuracy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic SVM\n",
    "As you can see the print_prediction function fits the SVM-Model to the training data and prints a very simple evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/svm/base.py:224: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training svm SVC(C=1.0, cache_size=2000, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=2000, probability=False, random_state=42, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "accuracy on traning data:\n",
      "0.830856617132\n",
      "accuracy on evaluation data:\n",
      "0.830146602932\n"
     ]
    }
   ],
   "source": [
    "print_prediction(training_data, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lets use different kernels and see how the perform\n",
    "Currently there are 3 Kernels implemented by sklearn that we can test. (Of course we could also implement own kernels later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/svm/base.py:224: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training svm SVC(C=1.0, cache_size=2000, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=2000, probability=False, random_state=42, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "accuracy on traning data:\n",
      "0.830383274332\n",
      "accuracy on evaluation data:\n",
      "0.830146602932\n",
      "linear\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/svm/base.py:224: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training svm SVC(C=1.0, cache_size=2000, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=2000, probability=False, random_state=42, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "accuracy on traning data:\n",
      "0.765271305426\n",
      "accuracy on evaluation data:\n",
      "0.722544450889\n",
      "sigmoid\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/svm/base.py:224: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training svm SVC(C=1.0, cache_size=2000, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='sigmoid',\n",
      "  max_iter=2000, probability=False, random_state=42, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "accuracy on traning data:\n",
      "0.638481341055\n",
      "accuracy on evaluation data:\n",
      "0.57537400748\n",
      "poly\n",
      "start training\n"
     ]
    }
   ],
   "source": [
    "for kernel in kernels:\n",
    "    print(kernel)\n",
    "    print_prediction(training_data, eval_data, kernel=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel seems to perform slightly better then the linear kernel. Both are far better then the sigmoid-kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets also check, if scaling the data improves the classification performance\n",
    "Luckily sklearn also provides features for scaling data, the standart scaling function from sklearn scales the data in a way that variance == 1 and mean == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data_scaled = [scaler.transform(training_data[0]), training_data[1]]\n",
    "eval_data_scaled = [scaler.transform(eval_data[0]), eval_data[1]]\n",
    "for kernel in kernels:\n",
    "    print(kernel)\n",
    "    print_prediction(training_data_scaled, eval_data_scaled, kernel=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parameter Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for kernel in kernels:\n",
    "    for C in np.logspace(-2, 0, num=5):\n",
    "        print_prediction(training_data, eval_data, kernel=kernel, C=C)\n",
    "\n",
    "for kernel in kernels:\n",
    "    for gamma in np.logspace(-2, 0, num=5):\n",
    "        print_prediction(training_data, eval_data, kernel=kernel, gamma=gamma)\n",
    "\n",
    "for kernel in ['poly', 'sigmoid']:\n",
    "    for coef0 in  np.logspace(-2, 0.5, num=6):\n",
    "        print_prediction(training_data, eval_data, kernel=kernel, coef0=coef0)\n",
    "\n",
    "for kernel in kernels:\n",
    "    for decision_function_shape in ['ovo', 'ovr']:\n",
    "        print_prediction(training_data, eval_data, kernel=kernel, decision_function_shape=decision_function_shape)\n",
    "        \n",
    "for kernel in ['poly']:\n",
    "    for degree in range(2, 5):\n",
    "        print_prediction(training_data, eval_data, kernel=kernel, degree=degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Findings\n",
    "## Influnce of number of training data\n",
    "The more training data is presented, the less likeli"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
