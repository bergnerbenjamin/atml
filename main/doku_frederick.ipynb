{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Approach\n",
    "\n",
    "Our approach is based on several different algorithms. We were very unfamiliar with the proposed data, to achieve good results anyway we decide to implement four algorithms instead of two. A short description of them is given in the following. Also their abilities to be used as on online or offline variant are mentioned.\n",
    "\n",
    "## K-Nearest Neighbour Algorithm (KNN)\n",
    "\n",
    "A very simple classifier, which is based on the k-nearest neighbours of a data point. When a new data point should be classified, all its labeled k nearest neighbours are taken into account. The data point is assigned to the class to which most of its k nearest neighbours are assigned.\n",
    "\n",
    "The algorithm can be used as online of offline variant. There are no differences between those variants, the result is the same in both cases.\n",
    "\n",
    "## Support-Vector-Machine SVM\n",
    "\n",
    "SVMs try to seperate classes in an N-dimensional space with a N-1-dimensional hyperplane. Because the data is not linear separable in most cases, the data is transformed to another space in which it is linear separable. This mapping is done with a kernel function.\n",
    "\n",
    "SVMs have several different parameters which have to be adapted to achieve a good result. SVMs are an offline classification method, all the data has to be avaibalbe in advance to classify new data points.\n",
    "\n",
    "## Decition Trees\n",
    "\n",
    "Decision Trees are a classification method which is based on trees from the field of graph theory. Every node distinguishes the dataset into different subsets. Leaf nodes contain no condition but a class label which is assigned to the data point. To classify a data point, a top-down traversal trough the tree is performed. Which child is selected depends on the condition of the actual node and the attribute value of the data point which has to be assigned. \n",
    "\n",
    "The tree has to be created before it can be used to classify new data points, it is an offline approach.\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes is a simple classifier which is based on the conditionally independence of all considered attributes. When this assumption is not fulfilled, this classifier leads to bad results.\n",
    "\n",
    "The algorithm is implemented as online or offline variant.\n",
    "\n",
    "# 6 Evaluation (Quality Measures)\n",
    "\n",
    "The overall task is a classification problem with six distinct classes. To compare our algorithms we use some quality measures which will be described in this chapter.\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "The accuracy is a common quality measure for classification problems. It is the number of right classified instances divided by the overall number of instances. This was our main quality measure for the comparison.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The confusion matrix is used to get a better overview over a single classification result. For N considered classes, the confusion matrix is an NxN matrix. The rows depict the ground truth, the columns the estimated class. Every cell of the matrix „counts“ classifications. For example a 100 in the cell (A,B) means that instances with the real class A, in 100 cases are assigned to the class B by the considered classifier. \n",
    "The desired result is a matix in which everything is zero except the values in the main diagonal. This means that all estimated classes are equal to the ground truth, for every instance. \n",
    "\n",
    "The confusion matrix helps to get a overview on how the different classes were separated. When two different classes A and B are confused, then the cells (A,B) and (B,A) will have high values. It is a useful visualization of a classification result to see the atony and strength of the corresponding classifier.\n",
    "\n",
    "We introduce an additional row (N+1) and column (N+1), in which the corresponding values of the row or column are summarized.\n",
    "\n",
    "# 8 Future Work\n",
    "\n",
    "The first aspect of the future work woulde be the improving of the implemented algorithms. Parameter tuning would be one of this tasks. With SVMs we achieve quite good results, but there is still room for improvement. Due to the limited time we could not test the data with enough different paramerters. Maybe Evolutionary Algorithms help to find the optimum of the available parameters.\n",
    "\n",
    "The second task is based on the preprocessing of the available data. The instances of the data are values which were computed by the accelerator and gyroskope in a very small time intervall. It is very unlikely that the class changes during 100 or 1000 consecutive instances. Approaches which combine a number of consecutive instances would maybe lead to better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
